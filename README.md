# Paper-Implementation-Overview-Gradient-Descent-Optimization-Algorithms  

[![forthebadge made-with-python](http://ForTheBadge.com/images/badges/made-with-python.svg)](https://www.python.org/) 


## "An Overview of Gradient Descent Optimization Algorithms"   
##  - Sebastian Ruder  

## Python 2.7  

Links to original paper published on arXiv.org>cs>arXiv:1609.04747  : [[1]](https://arxiv.org/abs/1609.04747), [[2]](https://arxiv.org/pdf/1609.04747.pdf)  

Implemented following Gradient Desent Optimization Algorithms from Scratch:  
1. Vanilla Batch/Stochastic Gradient Descent [[3]](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
2. Momentum  [[4]](https://www.cs.toronto.edu/~fritz/absps/momentum.pdf) [[5]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.5612&rep=rep1&type=pdf)  
3. Nesterov Accelarated Gradient  [[6]](https://www2.cs.uic.edu/~zhangx/teaching/agm.pdf)
4. Adagrad  [[7]](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
5. Adadelta  [[8]](https://arxiv.org/abs/1212.5701)
6. RMS Prop  [[9]](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
